{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49gpGJDVfseE"
   },
   "source": [
    "You can download the `requirements.txt` for this course from the workspace of this lab. `File --> Open...`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxHVyX3vfseE"
   },
   "source": [
    "# Build a Crew to Tailor Job Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ek6ew0xPfseE"
   },
   "source": [
    "If you are running this notebook on your own machine, you may need to install the required libraries:\n",
    "\n",
    "- `crewai`: A library for creating and managing AI agents and tasks.\n",
    "- `langchain`: A library for building applications with language models.\n",
    "- `langchain_community`: Community-contributed extensions for LangChain.\n",
    "- `openai`: OpenAI's API client library.\n",
    "- `boto3`: The Amazon Web Services (AWS) SDK for Python.\n",
    "- `python-docx`: A library for creating, modifying, and extracting information from Microsoft Word documents.\n",
    "- `langchain_aws`: AWS-specific extensions for LangChain.\n",
    "- `crewai_tools`: Tools for enhancing the functionality of CrewAI.\n",
    "- `anthropic`: A library for interacting with Anthropic's language models.\n",
    "- `litellm`: A lightweight library for interacting with various language models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 71389,
     "status": "ok",
     "timestamp": 1737742199115,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "id": "u7w7MYK2gBgY",
    "outputId": "d8763fee-d47e-4f05-e954-3095d2880492"
   },
   "outputs": [],
   "source": [
    "# These libraries can be installed only once after creating the Python virtual environment (venv).\n",
    "# Uncomment the lines below to install the libraries.\n",
    "\n",
    "#%pip install crewai\n",
    "\n",
    "#%pip install docx2txt\n",
    "\n",
    "#%pip install --upgrade langchain langchain-community \n",
    "\n",
    "#%pip install openai==1.75.0 \n",
    "\n",
    "#%pip install boto3\n",
    "\n",
    "#%pip install python-docx\n",
    "\n",
    "#%pip install langchain_aws\n",
    "\n",
    "#%pip install crewai_tools \n",
    "\n",
    "#%pip install langchain_community\n",
    "\n",
    "#%pip install anthropic \n",
    "\n",
    "#%pip litellm langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1737742255070,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 64,
    "id": "-mnGMezlfseF"
   },
   "outputs": [],
   "source": [
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Activating logging for debugging\n",
    "import logging\n",
    "boto3_logger = logging.getLogger(\"boto3\")\n",
    "boto3_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "botocore_logger = logging.getLogger(\"botocore\")\n",
    "botocore_logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAJCXdETfseF"
   },
   "source": [
    "- Import from the crewAI libray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6837,
     "status": "ok",
     "timestamp": 1737742248097,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 30,
    "id": "48X9nr0efseF",
    "outputId": "4d5b32e4-96cf-4cb8-8712-b82fff12a670"
   },
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTpmQTxvfseF"
   },
   "source": [
    "- As an LLM for your agents, you'll be using Haiku via Bedrock. This setup leverages the Bedrock platform to access the Haiku model, which will be used by the agents to perform their tasks effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the Serper API Key\n",
    "import sys\n",
    "import os\n",
    "from utils import get_serper_api_key, get_resume_file_path, pretty_print_result\n",
    "\n",
    "# Print the environment variables to validate\n",
    "print(\"SERPER_API_KEY: \", os.environ[\"SERPER_API_KEY\"])\n",
    "\n",
    "# Assuming your 'resume' folder is within the current working directory\n",
    "file_path = os.environ[\"RESUME_FILE_PATH\"]\n",
    "print(\"Resume Filepath:\", file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize a language model using the Amazon Bedrock. You can experiment with different models by changing the model parameter.\n",
    "\n",
    "from langchain_community.llms import Bedrock\n",
    " \n",
    "llm = LLM(model=\"bedrock/amazon.titan-text-lite-v1\", temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crewai_tools\n",
    "from crewai_tools import DOCXSearchTool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrewAI Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code initializes a semantic search tool using Amazon Bedrock as the LLM provider and embedder.\n",
    "semantic_search_resume = DOCXSearchTool(\n",
    "    docx=file_path,\n",
    "    config=dict(\n",
    "        llm=dict(\n",
    "            provider=\"aws_bedrock\",  # Using Amazon Bedrock as the LLM provider\n",
    "            config=dict(\n",
    "                model=\"amazon.titan-text-lite-v1\",  # Specify the Bedrock-supported model\n",
    "                temperature=0.6,\n",
    "                # top_p=1,\n",
    "                # stream=True,\n",
    "            ),\n",
    "        ),\n",
    "        embedder=dict(\n",
    "            provider=\"aws_bedrock\",  # Using Amazon Bedrock for embeddings\n",
    "            config=dict(\n",
    "                model=\"amazon.titan-embed-text-v2:0\",  # Bedrock-supported embedding model\n",
    "                task_type=\"retrieval_document\",\n",
    "                # title=\"Embeddings\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary tools from crewai_tools module and initialize them\n",
    "from crewai_tools import SerperDevTool, ScrapeWebsiteTool, FileReadTool\n",
    "\n",
    "search_tool = SerperDevTool()\n",
    "scrape_tool = ScrapeWebsiteTool()\n",
    "read_resume = FileReadTool(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code reads a .docx file and displays its content as a Markdown preview in a Jupyter notebook.\n",
    "\n",
    "from docx import Document\n",
    "\n",
    "def display_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])  # Extract text from all paragraphs\n",
    "    return text\n",
    "\n",
    "#file_path = \"resume/sample_resume.docx\"  # Upload your resume or use a sample \n",
    "resume_text = display_docx(file_path)\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f\"### Resume Preview\\n\\n{resume_text}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-ewaSMNfseF"
   },
   "source": [
    "## Creating Agents\n",
    "\n",
    "- Define your Agents, and provide them a `role`, `goal` and `backstory`.\n",
    "- It has been seen that LLMs perform better when they are role playing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1495,
     "status": "ok",
     "timestamp": 1737742689607,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 234,
    "id": "ImATBBrZfseG"
   },
   "outputs": [],
   "source": [
    "# Agent 1: Researcher\n",
    "researcher = Agent(\n",
    "    role=\"Tech Job Researcher\",\n",
    "    goal=\"Make sure to do amazing analysis on \"\n",
    "         \"job posting to help job applicants\",\n",
    "    tools = [scrape_tool, search_tool],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    backstory=(\n",
    "        \"As a Job Researcher, your prowess in \"\n",
    "        \"navigating and extracting critical \"\n",
    "        \"information from job postings is unmatched.\"\n",
    "        \"Your skills help pinpoint the necessary \"\n",
    "        \"qualifications and skills sought \"\n",
    "        \"by employers, forming the foundation for \"\n",
    "        \"effective application tailoring.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 284,
     "status": "ok",
     "timestamp": 1737742727663,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 370,
    "id": "6PAamrGhfseG"
   },
   "outputs": [],
   "source": [
    "# Agent 2: Profiler\n",
    "profiler = Agent(\n",
    "    role=\"Personal Profiler for Engineers\",\n",
    "    goal=\"Do increditble research on job applicants \"\n",
    "         \"to help them stand out in the job market\",\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    backstory=(\n",
    "        \"Equipped with analytical prowess, you dissect \"\n",
    "        \"and synthesize information \"\n",
    "        \"from diverse sources to craft comprehensive \"\n",
    "        \"personal and professional profiles, laying the \"\n",
    "        \"groundwork for personalized resume enhancements.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1737742742063,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 268,
    "id": "DoPUNk_wfseG"
   },
   "outputs": [],
   "source": [
    "# Agent 3: Resume Strategist\n",
    "resume_strategist = Agent(\n",
    "    role=\"Resume Strategist for Engineers\",\n",
    "    goal=\"Find all the best ways to make a \"\n",
    "         \"resume stand out in the job market.\",\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    backstory=(\n",
    "        \"With a strategic mind and an eye for detail, you \"\n",
    "        \"excel at refining resumes to highlight the most \"\n",
    "        \"relevant skills and experiences, ensuring they \"\n",
    "        \"resonate perfectly with the job's requirements.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent 4: Interview Coach\n",
    "interview_preparer = Agent(\n",
    "    role=\"Engineering Interview Coach\",\n",
    "    goal=\"Create interview questions and talking points \"\n",
    "         \"based on the resume and job requirements\",\n",
    "    tools = [scrape_tool, search_tool,\n",
    "             read_resume, semantic_search_resume],\n",
    "    verbose=True,\n",
    "    llm=llm,\n",
    "    backstory=(\n",
    "        \"Your role is crucial in anticipating the dynamics of \"\n",
    "        \"interviews. With your ability to formulate key questions \"\n",
    "        \"and talking points, you prepare candidates for success, \"\n",
    "        \"ensuring they can confidently address all aspects of the \"\n",
    "        \"job they are applying for.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXL9Cn_lfseG"
   },
   "source": [
    "## Creating Tasks\n",
    "\n",
    "- Define your Tasks, and provide them a `description`, `expected_output` and `agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1737743129860,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 268,
    "id": "KJjrOjfJfseG"
   },
   "outputs": [],
   "source": [
    "# Task for Researcher Agent: Extract Job Requirements\n",
    "research_task = Task(\n",
    "    description=(\n",
    "        \"Analyze the job posting URL provided ({job_posting_url}) \"\n",
    "        \"to extract key skills, experiences, and qualifications \"\n",
    "        \"required. Use the tools to gather content and identify \"\n",
    "        \"and categorize the requirements.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A structured list of job requirements, including necessary \"\n",
    "        \"skills, qualifications, and experiences.\"\n",
    "    ),\n",
    "    agent=researcher,\n",
    "    async_execution=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1737743135844,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 319,
    "id": "87ZN36cwfseG"
   },
   "outputs": [],
   "source": [
    "# Task for Profiler Agent: Compile Comprehensive Profile\n",
    "profile_task = Task(\n",
    "    description=(\n",
    "        \"Compile a detailed personal and professional profile \"\n",
    "        \"using the GitHub ({github_url}) URLs, and personal website \"\n",
    "        \"({personal_website}). Utilize tools to extract and \"\n",
    "        \"synthesize information from these sources.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A comprehensive profile document that includes skills, \"\n",
    "        \"project experiences, contributions, interests, and \"\n",
    "        \"communication style.\"\n",
    "    ),\n",
    "    agent=profiler,\n",
    "    async_execution=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1737743139365,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 166,
    "id": "gfbHnQ_3fseG"
   },
   "outputs": [],
   "source": [
    "# Task for Resume Strategist Agent: Align Resume with Job Requirements\n",
    "resume_strategy_task = Task(\n",
    "    description=(\n",
    "        \"Using the profile and job requirements obtained from \"\n",
    "        \"previous tasks, tailor the resume to highlight the most \"\n",
    "        \"relevant areas. Employ tools to adjust and enhance the \"\n",
    "        \"resume content. Make sure this is the best resume even but \"\n",
    "        \"don't make up any information. Update every section, \"\n",
    "        \"inlcuding the initial summary, work experience, skills, \"\n",
    "        \"and education. All to better reflrect the candidates \"\n",
    "        \"abilities and how it matches the job posting.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"An updated resume that effectively highlights the candidate's \"\n",
    "        \"qualifications and experiences relevant to the job.\"\n",
    "    ),\n",
    "    output_file=\"Tailored_Resume.md\",\n",
    "    context=[research_task, profile_task],\n",
    "    agent=resume_strategist,\n",
    "    async_execution=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task for Interview Preparer Agent: Develop Interview Materials\n",
    "interview_preparation_task = Task(\n",
    "    description=(\n",
    "        \"Create a set of potential questions and answers based on the profile and requirements of the position. \"\n",
    "        \"Use tools to generate questions and answers or potential discussion points that are relevant \"\n",
    "        \"to the candidate profile being sought for the position.\"\n",
    "Be sure to use these questions and discussion points to\n",
    "help the candidate highlight the main points of their resume\n",
    "and how they fit into the position.\"\n",
    "    ),\n",
    "    expected_output=(\n",
    "        \"A document containing key questions and answers to prepair candidates for job interviews, including \"\n",
    "        \"talking points that the candidate should prepare for the initial interview.\"\n",
    "    ),\n",
    "    output_file=\"Interview_Preparation.md\",\n",
    "    context=[research_task, profile_task, resume_strategy_task],\n",
    "    agent=interview_preparer,\n",
    "    async_execution=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "az9KWa4ffseG"
   },
   "source": [
    "## Creating the Crew\n",
    "\n",
    "- Create your crew of Agents\n",
    "- Pass the tasks to be performed by those agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE THIS FOR BEDROCK\n",
    "#from langchain_community.llms import Bedrock\n",
    " \n",
    "manager_llm = LLM(model=\"bedrock/amazon.titan-text-lite-v1\", temperature=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1737743155027,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 98,
    "id": "Z1MHmOIbfseH"
   },
   "outputs": [],
   "source": [
    "job_application_crew = Crew(\n",
    "    agents=[researcher,\n",
    "            profiler,\n",
    "            resume_strategist,\n",
    "            interview_preparer],\n",
    "\n",
    "    tasks=[research_task,\n",
    "           profile_task,\n",
    "           resume_strategy_task,\n",
    "           interview_preparation_task],\n",
    "    llm=manager_llm,\n",
    "    verbose=True,\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5BmxhUhfseH"
   },
   "source": [
    "## Running the Crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25632,
     "status": "ok",
     "timestamp": 1737744327194,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 30,
    "id": "m2fu3fPLfseH",
    "outputId": "58371bb7-4e6c-4c4c-e8f1-fdedcbbfda46"
   },
   "outputs": [],
   "source": [
    "job_application_inputs = {\n",
    "    'job_posting_url': 'https://openai.com/careers/software-engineer-backend/',\n",
    "    'github_url': 'https://github.com/lessandro-mendes-alhadas',\n",
    "    'personal_website': 'https://www.linkedin.com/in/lessandroalhadas/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this execution will take a few minutes to run\n",
    "result = job_application_crew.kickoff(inputs=job_application_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZPHAwV-fseH"
   },
   "source": [
    "- Display the results of your execution as markdown in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1737744343217,
     "user": {
      "displayName": "Viktoria Semaan",
      "userId": "13261563666975577364"
     },
     "user_tz": 480
    },
    "height": 47,
    "id": "3xaj_4R4fseH",
    "outputId": "80b75539-eb0d-42f3-ec24-f04c7a3b9842"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(result.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "height": 30,
    "id": "3R8WNNZwfseH"
   },
   "outputs": [],
   "source": [
    "#Let's estimate cost. You can see pricing at: https://aws.amazon.com/bedrock/pricing/\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "costs = (0.8 * (job_application_crew.usage_metrics.prompt_tokens) + 3.2*(job_application_crew.usage_metrics.completion_tokens) )/ 1000000\n",
    "print(f\"Total costs: ${costs:.4f}\")\n",
    "\n",
    "\n",
    "# Convert UsageMetrics instance to a DataFrame\n",
    "df_usage_metrics = pd.DataFrame([job_application_crew.usage_metrics.dict()])\n",
    "df_usage_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
